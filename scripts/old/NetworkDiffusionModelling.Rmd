---
title: "NetworkDiffusionModelling"
author: "Giuseppe Pontillo"
date: "27/6/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, python = reticulate::eng_python)
packages <- c("cocor","datawizard","dplyr","fmsb","fsbrain","ggExtra","ggeffects","ggpubr","ggplot2","ggseg","ggsegDKT","gridExtra","here","knitr","misty","MLmetrics","ms.sev","neurobase","nlme","pagedown","psycModel","ppcor","readr","tidyverse","survival","stargazer","R.matlab","NBR","plot.matrix","lattice","brainconn","reticulate","RColorBrewer","janitor")
lapply(packages, library, character.only = TRUE)
```

```{r import data, include=FALSE}
# import data
df_data <- read_csv(here("data/DatabaseCSF10y_long.csv"))
# import disconnection data
list <- list.files(path=here("./data/Schaefer100_Subcortex_nodes/"),
                   pattern="*.node", full.names=TRUE, recursive=FALSE)
disconnection_files <- lapply(list,read.table)
regions <- disconnection_files[[1]][,6]
df_node_disconnection <- data.frame(matrix(nrow = length(disconnection_files), ncol = length(regions)+1))
colnames(df_node_disconnection) = c("ID",paste0(regions,"_disc")) 
for (i in 1:length(disconnection_files)) {
  df_node_disconnection$ID[i] <- substr(basename(list[i]),1,15)
  df_node_disconnection[i,2:ncol(df_node_disconnection)] <- t(disconnection_files[[i]][,5]) 
}

# merge data
df <- merge(x = df_data, y = df_node_disconnection, 
            by.x = "StudyID_Session", by.y = "ID", all = TRUE)
df <- clean_names(df)
```

```{r transform data}
# z-score CT/volumes
volumes <- c(paste("left", c("amygdala",
                             "caudate",
                             "hippocampus",
                             "pallidum", 
                             "accumbens_area",
                             "putamen", 
                             "thalamus_proper"),  
                  sep = "_"), 
             paste("right", c("amygdala",
                             "caudate",
                             "hippocampus",
                             "pallidum", 
                             "accumbens_area",
                             "putamen", 
                             "thalamus_proper"),
                   sep = "_"))
CT <- colnames(clean_names(df_data))[grep("_thickness",colnames(clean_names(df_data)))]
for (i in volumes){
  MDL <- lm(get(i) ~ poly(age, degree = 2) + sex + e_tiv,
            data = df[df$group=="HC",], na.action =
              na.exclude)
  corrected <- df[,i] - predict(MDL, df)
  df[,paste0(i,"_zscore")] <- (corrected - mean(MDL$residuals)) /
    sd(MDL$residuals)
}
for (i in CT){
  MDL <- lm(get(i) ~ poly(age, degree = 2) + sex, 
            data = df[df$group=="HC",], na.action =
              na.exclude)
  corrected <- df[,i] - predict(MDL, df)
  df[,paste0(i,"_zscore")] <- (corrected - mean(MDL$residuals)) /
    sd(MDL$residuals)
}
# growth models
df_slopes <- data.frame(matrix(nrow = length(unique(df$study_id[df$group=="MS"])),
                               ncol = length(append(volumes,CT))+1))
colnames(df_slopes) = c("id",append(volumes,CT))
for (i in append(volumes, CT)){
  MDL <- lme(as.formula(paste0(i,"_zscore~fu_time")),
             data = df[df$group=="MS",],
             random = ~fu_time|study_id, method="ML", na.action =
                 na.exclude,
               correlation = corAR1(0, form = ~fu_time|study_id),
             control =list(msMaxIter = 1000, msMaxEval = 1000, sing.tol
                           = 1e-20))
  df_slopes$id <- row.names(coef(MDL))
  df_slopes[,i] <- coef(MDL)$fu_time
}
# get average maps
average_disc <- 
average_slope <- 
```

```{python}
from enigmatoolbox.datasets import load_sc, load_fc
from nilearn import plotting
import matplotlib.pyplot as plt
from enigmatoolbox.utils.parcellation import parcel_to_surface
from enigmatoolbox.plotting import plot_cortical
from enigmatoolbox.datasets import load_summary_stats
# Load summary statistics for ENIGMA-22q
sum_stats = load_summary_stats('22q')

# Get case-control cortical thickness and surface area tables
CT = sum_stats['CortThick_case_vs_controls']
SA = sum_stats['CortSurf_case_vs_controls']

# Extract Cohen's d values
CT_d = CT['d_icv']

SA_d = SA['d_icv']

# Map parcellated data to the surface
CT_d_fsa5 = parcel_to_surface(CT_d, 'aparc_fsa5')

# Project the results on the surface brain
plot_cortical(array_name=CT_d_fsa5, surface_name="fsa5", size=(800, 400),cmap='RdBu_r', color_bar=True, color_range=(-0.5, 0.5))

# Load cortico-cortical functional connectivity data
fc_ctx, fc_ctx_labels, _, _ = load_fc(parcellation='schaefer_100')

# Load cortico-cortical structural connectivity data
sc_ctx, sc_ctx_labels, _, _ = load_sc(parcellation='schaefer_100')

# Plot cortico-cortical connectivity matrices
fc_plot = plotting.plot_matrix(fc_ctx, figure=(9, 9), labels=fc_ctx_labels, vmax=0.8, vmin=0, cmap='Reds')
sc_plot = plotting.plot_matrix(sc_ctx, figure=(9, 9), labels=sc_ctx_labels, vmax=10, vmin=0, cmap='Blues')
```






